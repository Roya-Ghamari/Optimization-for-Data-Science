Frank-Wolfe and Pairwise Frank-Wolfe Project:

This project studies the application of the Frank-Wolfe (FW) algorithm together with its variant, Pairwise Frank-Wolfe (PFW), in minimizing the LASSO (Least Absolute Shrinkage and Selection Operator) regression problem. Convergence behavior and optimization efficiency of these algorithms are evaluated using two real-world datasets, which include Concrete Compressive Strength dataset and Boston Housing dataset. The LASSO problem, to be able to explicitly recover sparse solution in high-dimensional data, is an ideal method for feature selection. The results demonstrate that although FW algorithm is fast and decreases the duality gap and objective function rapidly, it tends to plateau early, resulting in suboptimal solutions. On the other hand, the PFW algorithm, although being more time consuming, produces more optimal solutions than the other algorithm by making more improvements constantly. It can be observed that the discrepancies in the results of FW and PFW are greater in Concrete dataset than in Boston Housing dataset, which demonstrate how the characteristics of a dataset can influence the performance of these optimization algorithms.

Gradient Descent and BCGD Project:

In this report, the application and comparison of two optimization algorithms, Gradient Descent (GD) and Block Coordinate Gradient Descent (BCGD) with the Gauss-Southwell rule, are investigated on multiple datasets, specifically Fashion-MNIST, Iris, and a custom-generated synthetic dataset. The focus is on multi-class logistic regression problems, where each method is applied to handle datasets with varying characteristics and complexity.
The synthetic dataset was created to systematically analyze the behaviors of GD and BCGD under controlled settings, adjusting parameters such as the learning rate and the number of iterations to refine performance. The Fashion-MNIST dataset, which consists of clothing items, serves as a more complex benchmark commonly used in machine learning to evaluate algorithm performance due to its diversity and real-world applicability. Additionally, the simpler and smaller Iris dataset is used to examine the scalability of these methods for problems with fewer features and classes.
Throughout the study, the experimental setup, implementation specifics, and results obtained are detailed, providing a comprehensive analysis of the performance differences between GD and BCGD. This includes insights into training efficiency, accuracy, and convergence behaviors of the loss function under varying conditions. The findings aim to contribute valuable insights into the effective application of these fundamental optimization techniques in machine learning, particularly in the context of multi-class logistic regression.
